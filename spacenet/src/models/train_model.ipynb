{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#!/usr/bin/env python\n",
        "\n",
        "#####################################################################################################################################################################\n",
        "# xView2                                                                                                                                                            #\n",
        "# Copyright 2019 Carnegie Mellon University.                                                                                                                        #\n",
        "# NO WARRANTY. THIS CARNEGIE MELLON UNIVERSITY AND SOFTWARE ENGINEERING INSTITUTE MATERIAL IS FURNISHED ON AN \"AS-IS\" BASIS. CARNEGIE MELLON UNIVERSITY MAKES NO    #\n",
        "# WARRANTIES OF ANY KIND, EITHER EXPRESSED OR IMPLIED, AS TO ANY MATTER INCLUDING, BUT NOT LIMITED TO, WARRANTY OF FITNESS FOR PURPOSE OR MERCHANTABILITY,          # \n",
        "# EXCLUSIVITY, OR RESULTS OBTAINED FROM USE OF THE MATERIAL. CARNEGIE MELLON UNIVERSITY DOES NOT MAKE ANY WARRANTY OF ANY KIND WITH RESPECT TO FREEDOM FROM PATENT, # \n",
        "# TRADEMARK, OR COPYRIGHT INFRINGEMENT.                                                                                                                             #\n",
        "# Released under a MIT (SEI)-style license, please see LICENSE.md or contact permission@sei.cmu.edu for full terms.                                                 #\n",
        "# [DISTRIBUTION STATEMENT A] This material has been approved for public release and unlimited distribution.  Please see Copyright notice for non-US Government use  #\n",
        "# and distribution.                                                                                                                                                 #\n",
        "# This Software includes and/or makes use of the following Third-Party Software subject to its own license:                                                         #\n",
        "# 1. SpaceNet (https://github.com/motokimura/spacenet_building_detection/blob/master/LICENSE) Copyright 2017 Motoki Kimura.                                         #\n",
        "# DM19-0988                                                                                                                                                         #\n",
        "#####################################################################################################################################################################\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "import chainer\n",
        "import chainer.functions as F\n",
        "import chainer.links as L\n",
        "from chainer import training\n",
        "from chainer.training import extensions\n",
        "\n",
        "from unet import UNet\n",
        "from dataset import LabeledImageDataset\n",
        "\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "import os\n",
        "\n",
        "\n",
        "def train_model():\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument('dataset', help='Path to directory containing train.txt, val.txt, and mean.npy')\n",
        "    parser.add_argument('images',  help='Root directory of input images')\n",
        "    parser.add_argument('labels',  help='Root directory of label images')\n",
        "    \n",
        "    parser.add_argument('--batchsize', '-b', type=int, default=16,\n",
        "                        help='Number of images in each mini-batch')\n",
        "    parser.add_argument('--test-batchsize', '-B', type=int, default=4,\n",
        "                        help='Number of images in each test mini-batch')\n",
        "    parser.add_argument('--epoch', '-e', type=int, default=50,\n",
        "                        help='Number of sweeps over the dataset to train')\n",
        "    parser.add_argument('--frequency', '-f', type=int, default=1,\n",
        "                        help='Frequency of taking a snapshot')\n",
        "    parser.add_argument('--gpu', '-g', type=int, default=0,\n",
        "                        help='GPU ID (negative value indicates CPU)')\n",
        "    parser.add_argument('--out', '-o', default='logs',\n",
        "                        help='Directory to output the result under \"models\" directory')\n",
        "    parser.add_argument('--resume', '-r', default='',\n",
        "                        help='Resume the training from snapshot')\n",
        "    parser.add_argument('--noplot', dest='plot', action='store_false',\n",
        "                        help='Disable PlotReport extension')\n",
        "\n",
        "    parser.add_argument('--tcrop', type=int, default=400,\n",
        "                        help='Crop size for train-set images')\n",
        "    parser.add_argument('--vcrop', type=int, default=480,\n",
        "                        help='Crop size for validation-set images')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    assert (args.tcrop % 16 == 0) and (args.vcrop % 16 == 0), \"tcrop and vcrop must be divisible by 16.\"\n",
        "\n",
        "    if args.gpu < 0:\n",
        "        from tboard_logger_cpu import TensorboardLogger\n",
        "    else:\n",
        "        from tboard_logger import TensorboardLogger\n",
        "\n",
        "    print('GPU: {}'.format(args.gpu))\n",
        "    print('# Minibatch-size: {}'.format(args.batchsize))\n",
        "    print('# Crop-size: {}'.format(args.tcrop))\n",
        "    print('# epoch: {}'.format(args.epoch))\n",
        "    print('')\n",
        "    \n",
        "    this_dir = os.path.dirname(os.path.abspath(__file__))\n",
        "    models_dir = os.path.normpath(os.path.join(this_dir, \"../../models\"))\n",
        "    log_dir = os.path.join(models_dir, args.out)\n",
        "    writer = SummaryWriter(log_dir=log_dir)\n",
        "    \n",
        "    # Set up a neural network to train\n",
        "    # Classifier reports softmax cross entropy loss and accuracy at every\n",
        "    # iteration, which will be used by the PrintReport extension below.\n",
        "    model = UNet()\n",
        "    if args.gpu >= 0:\n",
        "        # Make a specified GPU current\n",
        "        chainer.cuda.get_device_from_id(args.gpu).use()\n",
        "        model.to_gpu()  # Copy the model to the GPU\n",
        "\n",
        "    # Setup an optimizer\n",
        "    optimizer = chainer.optimizers.Adam()\n",
        "    optimizer.setup(model)\n",
        "    \n",
        "    # Load mean image\n",
        "    mean = np.load(os.path.join(args.dataset, \"mean.npy\"))\n",
        "    \n",
        "    # Load the MNIST dataset\n",
        "    train = LabeledImageDataset(os.path.join(args.dataset, \"train.txt\"), args.images, args.labels, \n",
        "                                mean=mean, crop_size=args.tcrop, test=False, distort=False)\n",
        "    \n",
        "    test = LabeledImageDataset (os.path.join(args.dataset, \"val.txt\"), args.images, args.labels, \n",
        "                                mean=mean, crop_size=args.vcrop, test=True, distort=False)\n",
        "\n",
        "    train_iter = chainer.iterators.SerialIterator(train, args.batchsize)\n",
        "    test_iter = chainer.iterators.SerialIterator(test, args.test_batchsize, repeat=False, shuffle=False)\n",
        "\n",
        "    # Set up a trainer\n",
        "    updater = training.StandardUpdater(\n",
        "        train_iter, optimizer, device=args.gpu)\n",
        "    trainer = training.Trainer(updater, (args.epoch, 'epoch'), out=log_dir)\n",
        "\n",
        "    # Evaluate the model with the test dataset for each epoch\n",
        "    trainer.extend(extensions.Evaluator(test_iter, model, device=args.gpu))\n",
        "\n",
        "    # Dump a computational graph from 'loss' variable at the first iteration\n",
        "    # The \"main\" refers to the target link of the \"main\" optimizer.\n",
        "    trainer.extend(extensions.dump_graph('main/loss'))\n",
        "\n",
        "    # Take a snapshot for each specified epoch\n",
        "    frequency = args.epoch if args.frequency == -1 else max(1, args.frequency)\n",
        "    trainer.extend(extensions.snapshot(), trigger=(frequency, 'epoch'))\n",
        "    \n",
        "    # Save trained model for each specific epoch\n",
        "    trainer.extend(extensions.snapshot_object(\n",
        "        model, 'model_iter_{.updater.iteration}'), trigger=(frequency, 'epoch'))\n",
        "\n",
        "    # Write a log of evaluation statistics for each epoch\n",
        "    trainer.extend(extensions.LogReport())\n",
        "\n",
        "    # Save two plot images to the result dir\n",
        "    if args.plot and extensions.PlotReport.available():\n",
        "        trainer.extend(\n",
        "            extensions.PlotReport(['main/loss', 'validation/main/loss'],\n",
        "                                  'epoch', file_name='loss.png'))\n",
        "        trainer.extend(\n",
        "            extensions.PlotReport(\n",
        "                ['main/accuracy', 'validation/main/accuracy'],\n",
        "                'epoch', file_name='accuracy.png'))\n",
        "\n",
        "    # Print selected entries of the log to stdout\n",
        "    # Here \"main\" refers to the target link of the \"main\" optimizer again, and\n",
        "    # \"validation\" refers to the default name of the Evaluator extension.\n",
        "    # Entries other than 'epoch' are reported by the Classifier link, called by\n",
        "    # either the updater or the evaluator.\n",
        "    trainer.extend(extensions.PrintReport(\n",
        "        ['epoch', 'main/loss', 'validation/main/loss',\n",
        "         'main/accuracy', 'validation/main/accuracy', 'elapsed_time']))\n",
        "\n",
        "    # Print a progress bar to stdout\n",
        "    trainer.extend(extensions.ProgressBar())\n",
        "    \n",
        "    # Write training log to TensorBoard log file\n",
        "    trainer.extend(TensorboardLogger(writer,\n",
        "        ['main/loss', 'validation/main/loss',\n",
        "         'main/accuracy', 'validation/main/accuracy']))\n",
        "    \n",
        "    if args.resume:\n",
        "        # Resume from a snapshot\n",
        "        chainer.serializers.load_npz(args.resume, trainer)\n",
        "\n",
        "    # Run the training\n",
        "    trainer.run()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train_model()\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}